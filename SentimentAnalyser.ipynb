{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "senior-visitor",
   "metadata": {
    "papermill": {
     "duration": 0.01735,
     "end_time": "2021-04-21T09:27:20.848668",
     "exception": false,
     "start_time": "2021-04-21T09:27:20.831318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sentiment Analysis Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numerical-decline",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:27:20.887746Z",
     "iopub.status.busy": "2021-04-21T09:27:20.885948Z",
     "iopub.status.idle": "2021-04-21T09:27:22.545825Z",
     "shell.execute_reply": "2021-04-21T09:27:22.544962Z"
    },
    "papermill": {
     "duration": 1.680196,
     "end_time": "2021-04-21T09:27:22.545991",
     "exception": false,
     "start_time": "2021-04-21T09:27:20.865795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-concentrate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:27:22.582742Z",
     "iopub.status.busy": "2021-04-21T09:27:22.582042Z",
     "iopub.status.idle": "2021-04-21T09:28:02.681238Z",
     "shell.execute_reply": "2021-04-21T09:28:02.680697Z"
    },
    "papermill": {
     "duration": 40.118513,
     "end_time": "2021-04-21T09:28:02.681377",
     "exception": false,
     "start_time": "2021-04-21T09:27:22.562864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading twitter_samples: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset and stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"twitter_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facial-munich",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:02.722886Z",
     "iopub.status.busy": "2021-04-21T09:28:02.722037Z",
     "iopub.status.idle": "2021-04-21T09:28:02.725242Z",
     "shell.execute_reply": "2021-04-21T09:28:02.724707Z"
    },
    "papermill": {
     "duration": 0.025656,
     "end_time": "2021-04-21T09:28:02.725402",
     "exception": false,
     "start_time": "2021-04-21T09:28:02.699746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bored-bulgarian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:02.765574Z",
     "iopub.status.busy": "2021-04-21T09:28:02.764964Z",
     "iopub.status.idle": "2021-04-21T09:28:03.696231Z",
     "shell.execute_reply": "2021-04-21T09:28:03.695535Z"
    },
    "papermill": {
     "duration": 0.953166,
     "end_time": "2021-04-21T09:28:03.696375",
     "exception": false,
     "start_time": "2021-04-21T09:28:02.743209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Divide into positive and negative tweets\n",
    "positive_tweets=twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets=twitter_samples.strings(\"negative_tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caroline-november",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:03.739952Z",
     "iopub.status.busy": "2021-04-21T09:28:03.738983Z",
     "iopub.status.idle": "2021-04-21T09:28:04.646998Z",
     "shell.execute_reply": "2021-04-21T09:28:04.647535Z"
    },
    "papermill": {
     "duration": 0.933126,
     "end_time": "2021-04-21T09:28:04.647728",
     "exception": false,
     "start_time": "2021-04-21T09:28:03.714602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-queue",
   "metadata": {
    "papermill": {
     "duration": 0.017478,
     "end_time": "2021-04-21T09:28:04.683461",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.665983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raised-irish",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:04.725311Z",
     "iopub.status.busy": "2021-04-21T09:28:04.724664Z",
     "iopub.status.idle": "2021-04-21T09:28:04.728014Z",
     "shell.execute_reply": "2021-04-21T09:28:04.728487Z"
    },
    "papermill": {
     "duration": 0.027127,
     "end_time": "2021-04-21T09:28:04.728659",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.701532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the preprocessing libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enormous-reducing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:04.773603Z",
     "iopub.status.busy": "2021-04-21T09:28:04.772907Z",
     "iopub.status.idle": "2021-04-21T09:28:04.775915Z",
     "shell.execute_reply": "2021-04-21T09:28:04.775282Z"
    },
    "papermill": {
     "duration": 0.029046,
     "end_time": "2021-04-21T09:28:04.776051",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.747005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A custom function to clean tweets\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    This function accepts as input, a string of unprocessed tweets and produces\n",
    "    preprocessed tweets as the output. '''\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # eliminate tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    clean_tweets = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            clean_tweets.append(stem_word)\n",
    "\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fatal-double",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:04.815635Z",
     "iopub.status.busy": "2021-04-21T09:28:04.815024Z",
     "iopub.status.idle": "2021-04-21T09:28:04.819743Z",
     "shell.execute_reply": "2021-04-21T09:28:04.820354Z"
    },
    "papermill": {
     "duration": 0.026189,
     "end_time": "2021-04-21T09:28:04.820563",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.794374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom function to check if tweets are preprocessed\n",
    "def lookup(function):\n",
    "    occurences = {('sad', 0): 4,\n",
    "             ('happy', 1): 12,\n",
    "             ('oppressed', 0): 7}\n",
    "    word = 'happy'\n",
    "    label = 1\n",
    "    if function(occurences, word, label) == 12:\n",
    "        return \"It works\"\n",
    "    return \"Doesnt work!!!!!!!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wrapped-gathering",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:04.861178Z",
     "iopub.status.busy": "2021-04-21T09:28:04.860558Z",
     "iopub.status.idle": "2021-04-21T09:28:04.865425Z",
     "shell.execute_reply": "2021-04-21T09:28:04.865990Z"
    },
    "papermill": {
     "duration": 0.026842,
     "end_time": "2021-04-21T09:28:04.866160",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.839318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    n = 0\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hundred-windsor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:04.910206Z",
     "iopub.status.busy": "2021-04-21T09:28:04.909564Z",
     "iopub.status.idle": "2021-04-21T09:28:04.911505Z",
     "shell.execute_reply": "2021-04-21T09:28:04.912128Z"
    },
    "papermill": {
     "duration": 0.027345,
     "end_time": "2021-04-21T09:28:04.912289",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.884944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def count_tweets(result, tweets, ys):\n",
    "   \n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in clean_tweet(tweet):\n",
    "            pair = (word,y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "awful-leadership",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:04.953371Z",
     "iopub.status.busy": "2021-04-21T09:28:04.952798Z",
     "iopub.status.idle": "2021-04-21T09:28:04.966059Z",
     "shell.execute_reply": "2021-04-21T09:28:04.966643Z"
    },
    "papermill": {
     "duration": 0.035406,
     "end_time": "2021-04-21T09:28:04.966822",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.931416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('joyou', 1): 1,\n",
       " ('trick', 0): 1,\n",
       " ('sad', 0): 1,\n",
       " ('hungri', 0): 1,\n",
       " ('tire', 0): 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the count_tweets() function\n",
    "\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am joyous', 'i am tricked', 'i am sad', 'i am hungry', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-blocking",
   "metadata": {
    "papermill": {
     "duration": 0.019336,
     "end_time": "2021-04-21T09:28:05.005376",
     "exception": false,
     "start_time": "2021-04-21T09:28:04.986040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "written-yellow",
   "metadata": {
    "papermill": {
     "duration": 0.018976,
     "end_time": "2021-04-21T09:28:05.043637",
     "exception": false,
     "start_time": "2021-04-21T09:28:05.024661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building the Model -Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handled-commissioner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:05.085105Z",
     "iopub.status.busy": "2021-04-21T09:28:05.084502Z",
     "iopub.status.idle": "2021-04-21T09:28:09.266408Z",
     "shell.execute_reply": "2021-04-21T09:28:09.266892Z"
    },
    "papermill": {
     "duration": 4.204209,
     "end_time": "2021-04-21T09:28:09.267070",
     "exception": false,
     "start_time": "2021-04-21T09:28:05.062861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fifth-brooklyn",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:09.310833Z",
     "iopub.status.busy": "2021-04-21T09:28:09.310172Z",
     "iopub.status.idle": "2021-04-21T09:28:09.319142Z",
     "shell.execute_reply": "2021-04-21T09:28:09.319717Z"
    },
    "papermill": {
     "duration": 0.03255,
     "end_time": "2021-04-21T09:28:09.319951",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.287401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    D_pos =np.sum(train_y)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
    "    D_neg = D-D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior =np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos =lookup(freqs,word,1)\n",
    "        freq_neg =lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assumed-moral",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:09.363543Z",
     "iopub.status.busy": "2021-04-21T09:28:09.362896Z",
     "iopub.status.idle": "2021-04-21T09:28:09.425174Z",
     "shell.execute_reply": "2021-04-21T09:28:09.424647Z"
    },
    "papermill": {
     "duration": 0.085151,
     "end_time": "2021-04-21T09:28:09.425326",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.340175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9089\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "confirmed-council",
   "metadata": {
    "papermill": {
     "duration": 0.019802,
     "end_time": "2021-04-21T09:28:09.465456",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.445654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Accutacy of the Naive Bayes is 90.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "drawn-wagner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:09.511797Z",
     "iopub.status.busy": "2021-04-21T09:28:09.511081Z",
     "iopub.status.idle": "2021-04-21T09:28:09.514698Z",
     "shell.execute_reply": "2021-04-21T09:28:09.514012Z"
    },
    "papermill": {
     "duration": 0.029236,
     "end_time": "2021-04-21T09:28:09.514902",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.485666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom function to test the accuracy of the classifier\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    word_l = clean_tweet(tweet)   \n",
    "    p = 0   # probablity starts at zero\n",
    "    p += logprior # increment probablity by the logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "narrow-longer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:09.561031Z",
     "iopub.status.busy": "2021-04-21T09:28:09.560265Z",
     "iopub.status.idle": "2021-04-21T09:28:09.573238Z",
     "shell.execute_reply": "2021-04-21T09:28:09.572334Z"
    },
    "papermill": {
     "duration": 0.03653,
     "end_time": "2021-04-21T09:28:09.573427",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.536897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.81 --> Negative sentiment\n",
      "1.8 --> Positive sentiment\n",
      "2.14 --> Positive sentiment\n",
      "0.0 --> Positive sentiment\n",
      "-1.83 --> Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Testing the accuracy of the classifier using synthetic tweets\n",
    "tweet = [\"Shit\", \"Awesome\", \"Great\", \"Dissapointed\", \"Poor\"]\n",
    "\n",
    "for item in tweet:\n",
    "    p=naive_bayes_predict(item, logprior, loglikelihood)\n",
    "    if p>=0:\n",
    "        print(round(p,2),\"--> Positive sentiment\")\n",
    "    elif p<=0:\n",
    "        print(round(p,2),\"--> Negative sentiment\")\n",
    "    \n",
    "\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dying-cisco",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:09.624939Z",
     "iopub.status.busy": "2021-04-21T09:28:09.624156Z",
     "iopub.status.idle": "2021-04-21T09:28:09.627713Z",
     "shell.execute_reply": "2021-04-21T09:28:09.627086Z"
    },
    "papermill": {
     "duration": 0.032959,
     "end_time": "2021-04-21T09:28:09.627867",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.594908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_classifier(test_x, test_y, logprior, loglikelihood):\n",
    "    \n",
    "    accuracy = 0 \n",
    "   \n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    \n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-queen",
   "metadata": {
    "papermill": {
     "duration": 0.020977,
     "end_time": "2021-04-21T09:28:09.670740",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.649763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing the accuracy of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "academic-anniversary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-21T09:28:09.721056Z",
     "iopub.status.busy": "2021-04-21T09:28:09.719992Z",
     "iopub.status.idle": "2021-04-21T09:28:10.785041Z",
     "shell.execute_reply": "2021-04-21T09:28:10.784338Z"
    },
    "papermill": {
     "duration": 1.093397,
     "end_time": "2021-04-21T09:28:10.785219",
     "exception": false,
     "start_time": "2021-04-21T09:28:09.691822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9940\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_classifier(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-lying",
   "metadata": {
    "papermill": {
     "duration": 0.022197,
     "end_time": "2021-04-21T09:28:10.830629",
     "exception": false,
     "start_time": "2021-04-21T09:28:10.808432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 56.80601,
   "end_time": "2021-04-21T09:28:11.561920",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-21T09:27:14.755910",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
